# -*- coding: utf-8 -*-
"""train_spam_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zumfVe13AxijaGX8CJ3rSqaorPw2QqZ_
"""



#TRAIN/TEST SPLIT# LOAD DATASET USING URL
from urllib.request import urlretrieve
import zipfile
import os
import pandas as pd
import json
import joblib
URL="https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"
urlretrieve(URL,"smsspamcollection.zip")
zip_ref=zipfile.ZipFile("smsspamcollection.zip","r")
zip_ref.extractall()
zip_ref.close()
os.remove("smsspamcollection.zip")

#LOAD INTO DATAFRAME
txt=pd.read_csv("SMSSpamCollection",sep="\t",header=None,names=["label","message"])
txt.head()
txt.info()
txt.keys()

#MAP LABELS TO INTEGERS
txt["label"]=txt["label"].map({"ham":0,"spam":1})

#STANDARDIZING TEXT
import string
txt['message']=txt['message'].str.lower().str.replace(r'[^\w\s]'," ", regex=True)
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(txt["message"],txt["label"],test_size=0.2,random_state=42)

#DEFINE VECTORIZER:
from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer
vect ={
    "count":CountVectorizer(stop_words='english',max_features=2000, ngram_range=(1,2)),
    "tfidf":TfidfVectorizer(stop_words='english',max_features=2000,ngram_range=(1,2))
}
#DEFINE MODELS:
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier
mod={
    "nb":MultinomialNB(),
    "lr":LogisticRegression(max_iter=2000, solver ='saga',C=1.0,random_state=42),
    "rf":RandomForestClassifier( n_estimators=200,
          max_depth=3,
          random_state=42),
    "gb":GradientBoostingClassifier( n_estimators=200,
                                   learning_rate=0.1,
                                    max_depth=3,
                                random_state=42)
}
#============================
#     SAVE MODELS
#============================
# ============================
# TRAIN & SAVE MODELS
# ============================

import os
os.makedirs("models", exist_ok=True)
os.makedirs("vectorizers", exist_ok=True)

for vec_name, vec in vect.items():

    # ✅ FIT vectorizer
    X_train_vec = vec.fit_transform(X_train)

    # ✅ SAVE fitted vectorizer
    joblib.dump(vec, f"vectorizers/{vec_name}.pkl")
    print(f"SAVED : vectorizers/{vec_name}.pkl")

    for mod_name, model in mod.items():

        # ✅ FIT model
        model.fit(X_train_vec, y_train)

        # ✅ SAVE trained model (unique name)
        joblib.dump(
            model,
            f"models/{mod_name}_{vec_name}.pkl"
        )
        print(f"SAVED : models/{mod_name}_{vec_name}.pkl")